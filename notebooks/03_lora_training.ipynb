{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Training: Mistral 7B for Structured JSON\n",
    "\n",
    "Fine-tune Mistral 7B using QLoRA to improve structured JSON output reliability.\n",
    "\n",
    "**Requirements**: Google Colab with GPU (T4 minimum, A100 recommended)\n",
    "\n",
    "**Time**: ~30-60 min on T4, ~10-15 min on A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes peft trl datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone repo and checkout lora-training branch\n",
    "import os\n",
    "if not os.path.exists('lora-support'):\n",
    "    !git clone https://github.com/aashnakunk/lora-support.git\n",
    "    %cd lora-support\n",
    "    !git checkout lora-training\n",
    "else:\n",
    "    %cd lora-support\n",
    "    print(\"Repo already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load training dataset\n",
    "TRAIN_PATH = \"data/train.jsonl\"\n",
    "\n",
    "train_data = []\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "print(\"\\nExample:\")\n",
    "ex = train_data[0]\n",
    "print(\"System:\", ex['messages'][0]['content'][:80])\n",
    "print(\"User:\", ex['messages'][1]['content'][:80])\n",
    "print(\"Assistant:\", ex['messages'][2]['content'][:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Format dataset for training (Mistral Instruct format)\n",
    "def format_example(example):\n",
    "    \"\"\"Format messages into Mistral Instruct template\"\"\"\n",
    "    messages = example['messages']\n",
    "    system = messages[0]['content']\n",
    "    user = messages[1]['content']\n",
    "    assistant = messages[2]['content']\n",
    "    \n",
    "    # Mistral Instruct format\n",
    "    text = f\"\"\"<s>[INST] {system}\n",
    "\n",
    "{user} [/INST]{assistant}</s>\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "formatted_data = [format_example(ex) for ex in train_data]\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(\"\\nFormatted example:\")\n",
    "print(dataset[0]['text'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load base model with 4-bit quantization\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Important for training\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare model for QLoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha (scaling factor)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to apply LoRA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nTrainable params: {trainable_params:,}\")\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-mistral-json\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"Training config:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Total steps: {len(dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING LORA TRAINING\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save LoRA adapter\n",
    "output_dir = \"./lora_adapter\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\nLoRA adapter saved to: {output_dir}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the fine-tuned model on a sample\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load eval data for quick test\n",
    "eval_data = []\n",
    "with open('data/eval.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        eval_data.append(json.loads(line))\n",
    "\n",
    "# Test on one example\n",
    "test_ex = eval_data[0]\n",
    "messages = test_ex['messages'][:2]\n",
    "prompt = f\"\"\"<s>[INST] {messages[0]['content']}\n",
    "\n",
    "{messages[1]['content']} [/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST OUTPUT (Fine-tuned LoRA Model)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUSER:\")\n",
    "print(test_ex['messages'][1]['content'])\n",
    "print(\"\\nMODEL OUTPUT:\")\n",
    "print(response)\n",
    "print(\"\\nEXPECTED:\")\n",
    "print(test_ex['messages'][2]['content'])\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Push to HuggingFace Hub\n",
    "\n",
    "If you want to save your adapter to HuggingFace Hub for easy loading later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# OPTIONAL: Push to HuggingFace Hub\n",
    "# Uncomment and run if you want to upload\n",
    "\n",
    "# !pip install -q huggingface_hub\n",
    "# from huggingface_hub import login\n",
    "# \n",
    "# # Login to HuggingFace (you'll need a token)\n",
    "# login()\n",
    "# \n",
    "# # Push adapter to Hub\n",
    "# HF_REPO = \"YOUR_USERNAME/mistral-7b-json-lora\"  # Change this\n",
    "# model.push_to_hub(HF_REPO)\n",
    "# tokenizer.push_to_hub(HF_REPO)\n",
    "# \n",
    "# print(f\"Adapter pushed to: https://huggingface.co/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Download** the `lora_adapter` folder from Colab\n",
    "2. **Zip it** and commit to your repo (or upload to HuggingFace Hub)\n",
    "3. Move to `lora-eval` branch to benchmark the fine-tuned model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
