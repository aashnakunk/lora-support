{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Evaluation: Mistral 7B + LoRA Adapter\n",
    "\n",
    "Evaluate the fine-tuned LoRA model and compare with baseline.\n",
    "\n",
    "**Run in Colab with GPU (T4 or better)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes peft torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone repo and checkout lora-eval branch\n",
    "import os\n",
    "if not os.path.exists('lora-support'):\n",
    "    !git clone https://github.com/aashnakunk/lora-support.git\n",
    "    %cd lora-support\n",
    "    !git checkout lora-eval\n",
    "else:\n",
    "    %cd lora-support\n",
    "    print(\"Repo already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from typing import Dict, List\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load eval dataset\n",
    "EVAL_PATH = \"data/eval.jsonl\"\n",
    "\n",
    "eval_data = []\n",
    "with open(EVAL_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        eval_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(eval_data)} eval examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load base model with 4-bit quantization\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading base model: {MODEL_NAME}...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\"Base model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load LoRA adapter\n",
    "# Option 1: If you uploaded adapter to HuggingFace Hub\n",
    "# ADAPTER_PATH = \"YOUR_USERNAME/mistral-7b-json-lora\"\n",
    "\n",
    "# Option 2: If you uploaded adapter folder to the repo\n",
    "ADAPTER_PATH = \"./lora_adapter\"  # Change this to your adapter location\n",
    "\n",
    "print(f\"\\nLoading LoRA adapter from: {ADAPTER_PATH}\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "print(\"LoRA adapter loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "INTENTS = [\"refund\", \"cancel\", \"billing\", \"tech_support\", \"shipping\", \"other\"]\n",
    "PRIORITIES = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "def is_valid_json(s: str) -> bool:\n",
    "    try:\n",
    "        json.loads(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_schema_compliant(s: str) -> bool:\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        required_keys = [\"intent\", \"priority\", \"entities\", \"needs_clarification\", \"clarifying_question\"]\n",
    "        if list(obj.keys()) != required_keys:\n",
    "            return False\n",
    "        if obj[\"intent\"] not in INTENTS:\n",
    "            return False\n",
    "        if obj[\"priority\"] not in PRIORITIES:\n",
    "            return False\n",
    "        if \"order_id\" not in obj[\"entities\"] or \"product\" not in obj[\"entities\"]:\n",
    "            return False\n",
    "        if not isinstance(obj[\"needs_clarification\"], bool):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def extract_json_from_text(text: str) -> str:\n",
    "    \"\"\"Try to extract JSON from markdown/text wrapper\"\"\"\n",
    "    text = re.sub(r'```json\\s*', '', text)\n",
    "    text = re.sub(r'```\\s*', '', text)\n",
    "    match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return text.strip()\n",
    "\n",
    "print(\"Validation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_inference(example: Dict, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"Run inference on a single example\"\"\"\n",
    "    messages = example['messages'][:2]  # system + user\n",
    "    \n",
    "    prompt = f\"\"\"<s>[INST] {messages[0]['content']}\n",
    "\n",
    "{messages[1]['content']} [/INST]\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Test on one example\n",
    "test_output = run_inference(eval_data[0])\n",
    "print(\"Test output:\")\n",
    "print(test_output)\n",
    "print(\"\\nValid JSON?\", is_valid_json(extract_json_from_text(test_output)))\n",
    "print(\"Schema compliant?\", is_schema_compliant(extract_json_from_text(test_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluation on full eval set\n",
    "EVAL_SIZE = 100  # Use 100 for quick test, 800 for full eval\n",
    "\n",
    "results = []\n",
    "valid_json_count = 0\n",
    "schema_compliant_count = 0\n",
    "intent_correct = 0\n",
    "total = 0\n",
    "\n",
    "print(f\"Running LoRA evaluation on {EVAL_SIZE} examples...\\n\")\n",
    "\n",
    "for i, example in enumerate(eval_data[:EVAL_SIZE]):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Progress: {i}/{EVAL_SIZE}\")\n",
    "    \n",
    "    expected = json.loads(example['messages'][2]['content'])\n",
    "    output = run_inference(example)\n",
    "    json_str = extract_json_from_text(output)\n",
    "    \n",
    "    valid_json = is_valid_json(json_str)\n",
    "    schema_valid = is_schema_compliant(json_str) if valid_json else False\n",
    "    \n",
    "    if valid_json:\n",
    "        valid_json_count += 1\n",
    "        predicted = json.loads(json_str)\n",
    "        \n",
    "        if schema_valid:\n",
    "            schema_compliant_count += 1\n",
    "            if predicted['intent'] == expected['intent']:\n",
    "                intent_correct += 1\n",
    "    \n",
    "    results.append({\n",
    "        'user_message': example['messages'][1]['content'],\n",
    "        'expected': expected,\n",
    "        'predicted_raw': output,\n",
    "        'predicted_json': json_str,\n",
    "        'valid_json': valid_json,\n",
    "        'schema_compliant': schema_valid\n",
    "    })\n",
    "    \n",
    "    total += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LORA EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total examples: {total}\")\n",
    "print(f\"Valid JSON: {valid_json_count}/{total} ({valid_json_count/total*100:.1f}%)\")\n",
    "print(f\"Schema compliant: {schema_compliant_count}/{total} ({schema_compliant_count/total*100:.1f}%)\")\n",
    "print(f\"Intent accuracy: {intent_correct}/{total} ({intent_correct/total*100:.1f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load baseline results for comparison\n",
    "try:\n",
    "    with open('baseline_results.json', 'r') as f:\n",
    "        baseline = json.load(f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON: Baseline vs LoRA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n{'Metric':<25} {'Baseline':<15} {'LoRA':<15} {'Improvement'}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    lora_valid_rate = valid_json_count / total\n",
    "    lora_schema_rate = schema_compliant_count / total\n",
    "    lora_intent_rate = intent_correct / total\n",
    "    \n",
    "    print(f\"{'Valid JSON Rate':<25} {baseline['valid_json_rate']*100:>6.1f}%  {lora_valid_rate*100:>11.1f}%  {(lora_valid_rate - baseline['valid_json_rate'])*100:>+8.1f}%\")\n",
    "    print(f\"{'Schema Compliance Rate':<25} {baseline['schema_compliance_rate']*100:>6.1f}%  {lora_schema_rate*100:>11.1f}%  {(lora_schema_rate - baseline['schema_compliance_rate'])*100:>+8.1f}%\")\n",
    "    print(f\"{'Intent Accuracy':<25} {baseline['intent_accuracy']*100:>6.1f}%  {lora_intent_rate*100:>11.1f}%  {(lora_intent_rate - baseline['intent_accuracy'])*100:>+8.1f}%\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\nbaseline_results.json not found - skipping comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Show sample improvements\n",
    "print(\"\\nSample outputs (LoRA):\")\n",
    "print(\"=\"*60)\n",
    "for i, r in enumerate(results[:3]):\n",
    "    print(f\"\\nEXAMPLE {i+1}:\")\n",
    "    print(f\"USER: {r['user_message'][:80]}...\")\n",
    "    print(f\"OUTPUT: {r['predicted_raw'][:150]}...\")\n",
    "    print(f\"Valid: {r['valid_json']}, Schema compliant: {r['schema_compliant']}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save LoRA results\n",
    "with open('lora_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'model': MODEL_NAME,\n",
    "        'adapter': ADAPTER_PATH,\n",
    "        'eval_size': total,\n",
    "        'valid_json_rate': valid_json_count / total,\n",
    "        'schema_compliance_rate': schema_compliant_count / total,\n",
    "        'intent_accuracy': intent_correct / total,\n",
    "        'detailed_results': results\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to lora_results.json\")\n",
    "print(\"Download this file to commit to your repo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
